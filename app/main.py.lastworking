# app/main.py
from fastapi import FastAPI, Request
import requests

app = FastAPI()

# Use localhost and the port where your model server is running
LLM_URL = "http://172.17.0.1:12434/engines/llama.cpp/v1/chat/completions"

@app.post("/chat")
async def chat(req: Request):
    data = await req.json()
    prompt = data.get("prompt")
    payload = {
        "model": "smollm2",  # make sure the model name matches
        "messages": [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": prompt}
        ]
    }

    try:
        response = requests.post(LLM_URL, json=payload, timeout=10)
        response.raise_for_status()
        return response.json()
    except requests.exceptions.RequestException as e:
        return {"error": str(e)}
